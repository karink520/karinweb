<!DOCTYPE html>
<html>
  <head>
    <title> Karin Knudson </title>
    <meta charset=UTF-8>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="css/kck.css">
    <link rel="stylesheet" href="css/blogpost.css">
    <link rel="stylesheet" href="css/dirichletprocessesd3.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.css" integrity="sha384-b/NoaeRXkMxyKcrDw2KtVtYKkVg3dA0rTRgLoV7W2df3MzeR1eHLTi+l4//4fMwk" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/jstat@latest/dist/jstat.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.js" integrity="sha384-ern5NCRqs6nJ/a4Ik0nB9hnKVH5HwV2XRUYdQl09OB/vvd1Lmmqbg1Mh+mYUclXx" crossorigin="anonymous"></script>
  </head>
  <body>
    <header>
      <div></div>
      <nav>
        <ul>
          <li> <a href="index.html"><i class="fas fa-home"></i></a></li>
          <li> <a href="blog.html">Blog</a> </li>
          <li> <a href="projects.html">Projects</a></li>
          <li> <a href="https://www.dropbox.com/s/jbhozp4n6fyvq0f/KarinKnudsonResume.pdf?dl=0">Resume</a><li>
        </ul>
      </nav>
    </header>
    <main>
      <div class=blogpost>
      <h3 class="blogdate">December 4, 2019 </h3>
      <span class="blogtopic" class="include-in-wordcloud"> Beta distributions, Dirichlet distributions and Dirichlet processes </span>
      <p>
        Today I am writing about some of my <em> favorite </em> distributions: the beta distribution,
        the Dirchlet distribution, and the Dirichlet process. All three are handy options
        to have at your disposal in specifying priors in certain Bayesian models, and they're also rather lovely
        in their own right.
      </p>
      <p>
        One thing that makes these distributions really neat is that they can be thought of as <em> distributions of
        distributions</em>.  That is, if we take a draw from one of these distributions, we
        get a result that itself fully describes a probability distribution.
        If we take another draw, we get another probability distribution.
        If we take a lot of draws, we get a whole bunch of distributions, with
        our beta distribution/Dirichlet distribution/Dirichlet process
        governing what kinds of distributions we might expect to see more or less commonly
        represented in the bunch.
      </p>
      <p>
       My goal today is to give some visual intuition for how these distributions of distributions work.
       Along the way, we'll also see examples of simple conjugate Bayesian analysis and hyperparameter interpretation for these distributions.
       We will begin with beta
       and Dirichlet distributions and build our way up to Dirichlet processes and beyond.
     </p>
       <button class="accordion">The beta distribution as a distribution of distributions?  Show me! </button>
       <div class="panel">
      <p>
        The probability distribution of a beta distribution is as follows:
      </p>
          <p id="betaPDFequation" class="centered-equation"> </p>

          <p> The beta distribution has support on the interval from 0 to 1, meaning a draw from a beta distribution will be a real number between 0 and 1.  The parameters
            <i>a</i> and <i>b</i> control the shape of the distribution.  The mean of a beta distribution
            is <span id="betaMeanequation"></span>.  For high values of <i>a</i> and <i>b</i>, the probability distribution will have a bump shape.
            If <i>a</i> and <i>b</i> are below 1, it will have a U shape.  If <i> a = b = 1</i>, the beta distribution reduces to a uniform distribution.
          </p>
          <p>
            Because a sample drawn from the beta distribution will be between 0 and 1, we can think
            of each draw as the probability of success in a trial or sequence of trials with two possible outcomes: success and failure.
            For example, it could represent the probability of heads (success) on each independent toss of biased coin.  So, once we have a draw
            from a beta distribution, we can draw from the new distribution that our beta draw specifies.
            As we generate a number of draws, we'll have success or failure, with the probability of success on each independent draw equal to the sample from the beta distribution that we began with.
          </p>
          <p>
            Let's get a feel for that visually and interactively. Here is a graph of the probability distribution function of a beta distribution (with a few different options
            for hyperparameter values.  Try sampling from the beta distribution, then using that sample to
            draw from the distribution it represents.  Here, the green draws on the left represent success, the blue failure.
          </p>
        <div><p id="betaPDF"> </p>
        </div>
          <button id="sampleFromBeta"> Draw a sample from a beta distribution </button>
          <p id="betaSample"> . </p>
          <canvas id="betaStickBreakingChart" width="1000px" height="50"> </canvas>
          <button id="sampleFromBetaSample"> Draw samples from the distribution over blue and green that is specified by the
            sample from the beta distribution.
          </button>
          <div class='canvas-container'><canvas id="BetaHistogram" width="200px" height="200"></canvas></div>

      </div>
      <button class="accordion">Let's get Bayesian: how can I use a beta distribution as a prior?</button>
      <div class="panel">
        <p>
          The beta distribution is useful as a prior over the probability of for each trial in
          a bernoulli or binomial experiment. In this case the parameters <i>a</i> and <i>b</i>
          have a very elegant interpretation.
        </p>
        <p>
          Note that we can also refer to <i>a</i> and <i>b</i> in this
          context as <em>hyperparameters</em>, because they are parameters of the prior distribution.
        </p>
        <p>
          Let's say we are going to flip a coin that might be biased.  We'll count a 'success' as flipping heads.   We'd like to represent
          our prior knowledge about the probability of heads (success), a probability we'll denote <i>x</i>.
          For example, if we think that most coins are fair and so have some idea that ours will be too,
          we might select a prior that puts more weight on values near 0.5, for example by choosing the following
          beta distribution with <i>a = b = 5</i>.  If we have reason to believe that the coin will be biased
          towards heads, we could choose a beta distribution that reflects that; for example we could let
          <i>a = 2 </i> and <i>b = 5</i>.  So different values of <i>a</i> and <i>b</i> help us encode our prior beliefs.
        </p>
          Our plan is to flip this coin a few times, to observe some data.  Then, we'll use
          Bayes rule to compute the distribution that gives new idea of what <i>x</i>.  That is,
          we'll combine our <em> prior distribution</em> with the <em>likelihood</em> of some observed data,
          to get a <em>posterior distribution</em> for <i>x</i>, the parameter of interest.

          We flip the coin <i>n</i> times and see <i>k</i> heads, remembering that the probability for success on
          any given flip is our unknown <i>x</i>.  The probability of seeing <i>k</i> heads comes from the binomial distribution and is:
        </p>
        <p id="binomialPdfEquation" class="centered-equation"> </p>
        This probability of seeing certain data (here, the data is that we saw <i>k</i> successes in <i>n</i> trials) given the assumptions of our model is the <em>likelihood</em>.
        Bayes' rule tells us that the posterior distribution for <i>x</i> should be proportional to the product of the likelihood and the prior.
        That is:
        <p id="bayesRuleExampleEquation" class="centered-equation"> </p>

        Multiplying the binomial expression for likelihood with the beta prior for <i>x</i>, we obtain:

        <p id="posteriorBetaPdfEquation" class="centered-equation"> </p>

        If we include the normalization constant, we have as the posterior distribution for <i>x</i>:
        <p id="normalizedBetaPdfEquation" class="centered-equation"> </p>
      </div>

      <button class="accordion">Is that conjugacy?  And how do I interpret the hyperparameters (a,b)?</button>
      <div class="panel">
        <p>
        "Wow, that's another beta distribution!" you say, looking at the posterior distribution we just derived.  And you're right!
        </p>
        <p>
          We say that the beta distribution is the <em>conjugate</em> prior for the binomial distribution, because when we combine the
          likelihood and prior, we get a posterior that has the form of the prior again: another beta distribition.
        </p>

        <p>
        Not only that, we see that
        playing the role of the first parameter of the beta distribution in the posterior is <i>a + k</i>, that is <i>a</i> plus the number of successes we saw in the data.
        Playing the role of the second parameter is <i>b + n - k</i>, hyperparameter <i>b</i> plus the number of failures in the data.
        If we increased <i>a</i> by one in setting the prior, that would affect the posterior in the same way as seeing one more success in the data would have.
        If we increased <i>n</i> by one in setting the prior, that would affect the posterior in the same way as seeing one more failure in the data would have.
      </p>

      <p>
        Thus we can think of <i>a</i> and <i>b</i> as <em> pseudocounts </em> of successes and failures, respectively.
        We can think of our prior idea about the fairness of the coin is the same as the idea we would have had if our prior came from observing a binomial trial where we saw <i>a</i> successes and <i>b</i> failures.
        In light of this interpretation, those hyperparameters <i>a</i> and <i>b</i> are not so mysterious after all.
        Pretty neat!
      </p>

      <p>
        To tie these ideas back to a concrete example, let's say we put a beta prior over the probability of heads in our coin toss with <i>a = b = 5</i>.
        Our prior idea of the fairness of the coin is as if all we knew about it was that on 10 independent flips, we had seen
        5 heads and 5 tails - the pseudocounts. Then, suppose we toss the coin <i>n = 10</i> times and see <i>k = 9</i> heads.  Now we might have a little more
          suspicion that <i>x</i>, the probability of heads is more than 0.5.  In fact our uncertainty about <i>x</i>
          after seeing the data is captured by a beta distribution with parameters <i>a + k = 14 </i> and <i>b+ n - k = 6</i>.
          We plot these example prior (green) and posterior (blue) distributions below.
      </p>
      <div><p id="priorPosteriorBetaPlot"></p></div>
      <div><p id="priorPosteriorBetaPlot2"></p></div>

      </div>
      <button class="accordion">I'm ready for Dirichlet distributions.</button>
      <div class="panel">
        <p>
        A draw from beta distribution is great for describing the probability of outcomes when
        there are only two possible outcomes (e.g. success/failure or heads/tails).  If we'd like to
        have a distribution whose draws specify probability distributions over more than two outcomes,
        we can use the Dirichlet distribution, which is a generalization of the beta distribution.  In
        Bayesian modeling, Dirichlet distributions are handy for priors over multinomial or categorical variables, 
        for instance, you might use them to describe some prior idea above the relative weights of different
        mixing components in a mixture model.
      </p>
      <p>
         Each draw from a Dirichlet distribution is a vector of prespecified length <i>M</i> that sums to one.
         For example, (0.2, 0.3, 0.4, 0.1) could be a draw from a Dirichlet distribution with <i>M = 4</i>.
         We can view this vector a probability distribution over a discrete random variable with <i>M = 4</i> possible
         outcomes, the first occurring with probability 0.2, the second with probability 0.3, and so on.
         In place of the two parameters <i>a</i> and <i>b</i>, we have a vector of <i>M</i> parameters <span id='dirichletParameterEquation'> </span> that control
         the shape of a Dirchlet distribution, and thus what kinds of vectors of probabilities
         are likely or unlikeley to be drawn from it.

      </p>
      <p> The probability density function for a Dirichlet distribution is a follows:
      </p>
      <p id='dirichletPdfEquation' class="centered-equation"> </p>
      <p>
        Here are some illustrations of Dirichlet probability distributions for <i>M = 3</i>.  (Higher values of M get difficult to visualize on a screen!)
      </p>
        <figure>
            <img src='https://upload.wikimedia.org/wikipedia/commons/2/2b/Dirichlet-3d-panel.png' alt="plot of Dirichlet probability distributions">
                <br>
        <caption>Image from https://upload.wikimedia.org/wikipedia/commons/2/2b/Dirichlet-3d-panel.png</caption>
        </figure>
        <p> We can play the same game as before, sampling from a Dirichlet distribution and then using that sample to draw new samples from a discrete distribution, this time with more than two possible outcomes.</p>
        <button id="sampleFromDirichlet"> Draw a sample from a Dirichlet distribution with parameters (3, 4, 5, 1 ) </button>
        <p id="dirichletSample"> . </p>
        <canvas id="dirichletStickBreakingChart" width="1000px" height="50"> </canvas>
        <button id="sampleFromDirichletSample"> Draw samples from the distribution that is specified by the
            sample from the Dirichlet distribution </button>
          <div class='canvas-container'><canvas id="DirichletHistogram" width="400px" height="200"></canvas></div>
        <p>
        Once again we have conjugacy, and a helpful interpretation of hyperparameters as pseudocounts.
        Just as the beta distribution is the conjugate prior for the Bernoulli and binomial distributions,
        the Dirichlet distribution is the conjugate prior for the categorical and multinomial distributions.
        
        </p>
        <p>
        Suppose we have <i>n</i> independent trials, where the probability of each of the <i>M</i> possible outcomes occuring in each trial is given by the corresponding element of the <b><em>x</em></b>, where <b><em>x</em></b> is still <i>M</i>-dimensional and its elements sum to one.  We use a Dirichlet distribution to represent our prior knowledge about <b><em>x</em></b>. We will then observe data and update our idea of  <b><em>x</em></b>.  We represent a specific set of outcomes
        by the <i>M</i>-dimensional vector <b><em>k</em></b>, where each element of
        <b><em>k</em></b> is a whole number that gives the number of appearances of the corresponding outcomes in the <i>n</i> trials (so that the sum of all elements in <b><em>k</em></b> is <i>n</i>).  The probability of observing <b><em>k</em></b>
        follows a multinomial distribution (which generalizes the binomial distribution):
        <p id="multinomialPdfEquation" class="centered-equation"> </p>
        If we set a Dirichlet prior over the vector of probabilities <b><em>x</em></b>, and then combine it with some vector <b><em>k</em></b> representing data about the outcomes of <i>n</i> trials, we can multiply the two to get the following posterior distribution:
        </p>
        
        <p id='posteriorDirichletPdfEquation' class="centered-equation"></p>
        <p> Including the constant of normalization, we have: </p>
        <p id='normalizedPosteriorDirichletPdfEquation' class="centered-equation"></p>
        <p>
        So the posterior distribution is a Dirichlet distribution, just like the prior - we have conjugacy. Moreover, if we look at how the hyperparameters of  <b><em>a</em></b> add with counterparts in the vector of counts <b><em>k</em></b> from the data, we see the elements of <b><em>a</em></b> playing the same role of pseudocounts as the hyperparameters <i>a</i> and <i>b</i> did in the beta distribution.  Delightful!
        </p>
      </div>










      <button class="accordion">On to Dirichlet processes!</button>
      <div class="panel">
      <p>
      The theme of this post is distributions of distributions.  A sample from a Dirichlet process itself <em>almost surely</em>
      (in the technical sense of that expression) specifies a discrete probability distribution.  However, while a Dirichlet distribution
      yielded distributions over <i>M</i> outcomes, a Dirichlet process (DP) can yield distributions over infinitely many outcomes.
      As we did with beta distributions and Dirichlet distributions, we want to break down how we get to this sample
      that itself specifies a distribution, and then how we generate samples from that distribution. 
      </p>
      <p>
      A Dirchlet process is specified using two parameters, a positive real number &alpha;, and a probability distribution <i>H</i>.
      "Whoa," you say, "one of the 'parameters' is a whole probability distribution?"  That's right.  It's called the base distribution.
      </p>
      <p>
      <i>H</i> can be continuous or discrete.  To generate the visualization below, we've used a standard normal distribution for <i>H</i>, but you could just as easily use something else.
      You get to choose &alpha;, though:</p>
      
      <p> &alpha; = 
      <select id="choose-alpha" height=18px>
          <option value="0.5">0.5</option>
          <option value="1.2" selected="selected">1.2</option>
          <option value="3">3</option>
      </select>
    </p>
      <p id="choose-alpha"></p>
      </p>
      <p>One way to conceptualize a draw from a Dirichlet process is as a <em>stick-breaking process</em>. We start with 
      a stick of length one:
      </p>
      <canvas id="empty-stick" height=50px></canvas>
      </p>
      <p>Next, we draw a value from <i>H</i>.  We're going to pair that value with some probability mass.  The probability
        mass comes from a portion of the stick.  What portion of the stick?  A random portion, determined by a draw from
        a Beta(1, &alpha;) distribution.
      </p>
      <button id="break-the-stick">Break the stick</button>
      <p>Draw from Beta(1, &alpha;): <span class="stick-breaking-draw-1"></span></p>

      <canvas id="stick-after-one-step" height=50px></canvas>
      <p>
        Now we draw another value from <i>H</i>.  We pair it with a portion of the probability that is not yet allocated (the portion
        of the stick that is not yet colored in, above).  What portion of what's left?  A random portion, determined by a draw from
        a Beta(1, &alpha;) distribution.
      </p>
      <button id="break-the-stick-again" >Break the stick again</button>
      <p>Draw from Beta(1, &alpha;): <span class="stick-breaking-draw-2"></span>. <br> The probability mass that gets associated with the second draw from <i>H</i>
      is  (1-<span class="stick-breaking-draw-1">(first beta draw)</span>)&times; <span class="stick-breaking-draw-2">(second beta draw)</span> <span id="second-break"></span>.
    </p>
      <canvas id="stick-after-two-steps" height=50px></canvas>
      <p>
        We continue in this way for infinitely many discrete steps, drawing a value from <i>H</i> and associating it with a certain probability mass.
        The result is a discrete distribution over our (countably infinite) draws from <i>H</i>.  The support is given by set of draws we made from <i>H</i>,
        which we'll denote by {<span id="thetaN"></span>}, and the
        and the probability for each element in the support comes from the corresponding portion of the broken stick, which we will denote {<span id="betaN"></span>}
        (noting that they sum to one).
        We can thus represent the result of this procedure, which represents one draw from the Dirichlet process <i>DP(&alpha;,H)</i> with the
        following mathematical expression:
      </p>

      <p id="stickBreakingEquation" class="centered-equation"></p>
    
      <p>Here &delta; is the dirac delta.  Because each of the portions of the stick were sampled from Beta(1, &alpha;), we can see
        that decreasing &alpha; will cause us to allocate larger chunks of the stick to begin with.  Thus, we can get a sense for how
        &alpha; controls the concentration of a draw from the Dirichlet process: smaller values of &alpha; lead to higher concentrations of probability mass on a few elements.
      </i></p>
      <p>
        Incidentally, the distribution over sequences of &beta; generated by this stick breaking procedure, parametrized by &alpha;, is called the GEM distribution.
        The name GEM comes from the initials of the authors Griffiths, Engen, and McCloskey.
         <!--cite GEM distribution-->
        </p>
      <p>
        During our discussions of the the beta and Dirichlet distributions, we used a sample from the beta or Dirichlet distribution to draw from the 
        distribution it specified.  We could imagine doing that with the stick breaking process - using the stick breaking 
        process to tell us the probability given to each of the infinitely many dicrete draws from <i>H</i>, and generating samples 
        from there.  But, we can actually generate samples the distribution given by a draw from a Dirichlet process
        <em>without ever specifying</em> what that draw was.  Bypassing an explicit representation of the initial 
        draw can help us deepen our understanding, and it is also a useful view for a number of computations (e.g. using Gibbs sampling for inference).
      </p>
      <p>
        To begin a metaphor, imagine an empty restaurant.  A person walks in, and sits at a table.  They find out that everyone
        at that table is to be served a random "dish".  It's not a real dish, but actually just a random draw from the distribution 
        <i>H</i>.  Yum.
      </p>
      <p>
         More people walk in, one at a time, and settle in at more tables, each of which is associated with its own
         independently drawn sample from <i>H</i>.  The key is in how each person decides where to sit.  With probability proportional
         to &alpha;, a person will boldly strike out on their own and choose a new table.  Alternatively, they will choose to 
         sit at an occupied table with probability proportional to the number of people who are already there
         (we imagine them reasoning that if other people chose a table, it must be good, so joining an already popular table seems appealing).
         Normalizing these probabilities properly, the <i>n</i>th person choose a new table (associated with a new draw from <i>H</i>)
         with a probability of <span id="newTableProbability"></span> and choose an existing table with probability <span id="existingTableProbability"></span>,
         where <span id=nT></span> is the number of people already at table <i>t</i>.  You'll see this process described as the "Chinese Restaurant Process" in some of the literature.
      </p><p>
         Below, we visualize this generation of samples based on a single draw from a DP. The top graph shows <i>H</i>.  We'll mark up the graph of <i>H</i> as we
         make independent draws from it to associate with each new table.  Below, we see the simulation 
         of people choosing a table, where each table is associated with an independent draw 
         from <i>H</i>.  The number of tables will continue to grow, and we can't show all of them, so here we're only picturing the first few. 
         (Any samples from the base distribution that are marked in gray are from tables that are not shown.)
        
        </p>
      </p>
      <div><button id="sampleFromDPSample"> Draw from the distribution specified by a realization of a Dirichlet process</button></div>
      
      <button id="startOverDP">Start over</button>
      <p id="normalPDF"> </p>
      <div>
      <canvas id="circle" width="600px" height="100"></canvas>
      </div>
      <div>
      <canvas id="DPhistogram" width="600px" height="200"></canvas>
      </div>
      <p>
        What about conjugacy? Like the beta and Dirichlet distributions, Dirichlet processes also have some nice conjugacy properties. 
        In particular, if <i>G ~ DP(&alpha;, H)</i>, and we have a set of observations <span id="xN"></span></span><i> ~ G</i> (i.i.d.), then 
        the postierior distribution for <i>G</i> is also a Dirichlet process:
        <p id="DPConjugacyDirect"></p>
        We can see that each observation updates the base distribution with additional probability mass according to the observation, in the same way
        that we updated the psuedocount parameters of beta and Dirichlet distributions by adding in counts of observations.
        <p> One can futher derive the predictive distribution, which should remind us of the table selection process above:</p>
        <p id="DPConjugacy"></p>
      </p>
      <p>
          When might you use a DP?
          To touch on the practical utility of Dirichlet processes in modeling, it can be helpful to note the limitations of a Dirichlet distribution.
          We noted that Dirichlet distributions are useful in specifying priors for, e.g. the weights of different components in a 
          mixture model, when we know in advance the number of components (<i>M</i>, in the notation above).
          However, it is very often the case that we do not know in advance how many mixture components we have, and that in fact
          as we observe more data, we expect to continue to accumulate more components. 
          For example, suppose we are modeling mixtures of topics in some natural language corpus; we do not know how many topics to expect,
          and as we see more data, we might need to add more topics to adequately descibe it.  Or, perhaps we are looking for subtypes of a disease
          based on clusters in the disease characteristics, but we do not know how many subtypes there might be.  In such cases, a Dirichlet process may
          fit nicely with our intuition that as we observe more data, we may also accumulate more mixture components (tables, in the restaurant metaphor),
          and the draws from base distribution may be used in parameterizing the components themselves.  It's a handy tool to have at your disposal!
        </p>
      <p>
        Phew!  That's it on DPs for now.  But in thinking about DPs as priors, we just dipped our toe into Bayesian nonparametrics - there is a whole world out 
        there to explore if you want!
      </p>
    </div>
    <button class="accordion">But what is a Dirichlet process, really?  Also, references? </button>
    <div class="panel">
      <p>
      Look, it took a real battle against my instincts as a mathematician not to just start with the formal definition and assume everything was clear from there. But I'm so glad you asked.</p>
      <p>
      Let <i>(X, B)</i> be a measurable space, with <i>H</i> a probability measure on that space.  Then the stochastic process G indexed by the
      sets in <i>B</i> is a Dirichlet process distributed as <i>DP(&alpha;, H)</i> if for any finite measurable partition <span id="setlist"></span> of <i>X</i>, the random vector
      <span id="setlist2"></span> has a Dirichlet distribution with parameter vector <span id="setlist3"></span>.
      </p>
      <p>
        "Just writing down a definition like that doesn't guarantee that <i>G</i> exists," you might say.  Fantastic point!  The existence
        of the Dirichlet process was proven in <b><a href="https://projecteuclid.org/euclid.aos/1176342360">[Ferguson, 1973]</a></b>.
      </p>
      <p>
          The explicit connection to the Dirichlet distribution given by the definition can help us better understand how both the stick-breaking 
          process and the Chinese restaurant process represent a Dirichlet process.  A full proof is beyond the scope of this post, but to point
          to some rough intuition, we note that the relationship to the Dirichlet distribution can be used to derive the conjugacy relations and predictive distribution
          in the previous section, which gives rise to the table-choosing proportions of the restaurant process.
          For the stick-breaking construction, note that since a Dirichlet process corresponds to a Dirichlet distribution over any finite partition of <i>X</i>,
          then if we partition <i>X</i> into, say, the set containing our first draw from <i>H</i> and its complement, that should follow a Dirichlet
          distribution with <i>M=2</i>, i.e. a beta distribution.  For rigor and a deeper understanding, we refer the reader to  <b><a href="http://www3.stat.sinica.edu.tw/statistica/oldpdf/A4n216.pdf">[Sethuraman 1994]</a></b>,
          which establishes the stick-breaking construction of a beta process, <b><a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1176342372">[Blackwell and MacQueen, 1973]</a></b>,
          which presents a Polya urn scheme closely related to the Chinese Restaurant process, and the exposition in Yee Why Teh's excellent <b><a href="href='https://www.stats.ox.ac.uk/~teh/teaching/npbayes/mlss2007.pdf">[Tutorial]</a></b>.
        </p>
        <p>
            Another foundational paper in the realm of DPs is <b><a href="https://projecteuclid.org/euclid.aos/1176342360">[Antoniak, 1974]</a></b>, which explored theory 
            and applications of mixtures of Dirichlet processes.
          </p>
      <p>
        For a somewhat more recent exposition about Dirichlet processes, see, for example <b><a href="https://www.stat.berkeley.edu/~aldous/206-Exch/Papers/hierarchical_dirichlet.pdf">[Teh et al., 2006]</a></b>.  This paper introduces
        hierarchical Dirichlet process, which will be the topic of an upcoming blogpost.
      </p>
      <ol>
        <li class="reflist"><a href="https://projecteuclid.org/euclid.aos/1176342871">Antoniak, Charles E. "Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems." The annals of statistics (1974): 1152-1174.</a></li>
        <li class="reflist"><a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1176342372">Blackwell, David, and James B. MacQueen. "Ferguson distributions via PÃ³lya urn schemes." The annals of statistics 1.2 (1973): 353-355.</a></li>
        <li class="reflist"><a href="https://projecteuclid.org/euclid.aos/1176342360">Ferguson, Thomas S. "A Bayesian analysis of some nonparametric problems." The annals of statistics (1973): 209-230.</a></li>  
        <li class="reflist"><a href="http://www3.stat.sinica.edu.tw/statistica/oldpdf/A4n216.pdf">Sethuraman, Jayaram. "A constructive definition of Dirichlet priors." Statistica sinica (1994): 639-650.</a> </li>
        <li class="reflist"><a href="https://www.stat.berkeley.edu/~aldous/206-Exch/Papers/hierarchical_dirichlet.pdf">Teh, Yee W., et al. "Hierarchical Dirichlet processes."  Journal of the American Statistical Association (2006): 1566-1581</a></li>
        <li class="reflist"><a href="href='https://www.stats.ox.ac.uk/~teh/teaching/npbayes/mlss2007.pdf">Teh, Yee W., Dirichlet Processes: Tutorial and Practical Course (2007)</a></li>
      </ol>
    </div>
    <button class="accordion">In conclusion...</button>
    <div class="panel">
      <p>
      These distributions are cool in their own right, and I've
      found them useful in applied Bayesian modeling.  The interpretation of the
      hyperparameters in terms of pseudocounts has a lovely logic to it. I know I found this interpretation soothingly concrete
      when I was getting started with Bayesian modeling, back when choosing hyperparameters seemed quite mysterious.  I hope you
      find some elegant and useful applications of these distributions too.
      </p>
      <p>
      Lastly, stay tuned.  An additional motivation for writing this post was
      to lay the groundwork for a discussion of a related but more complex topic: hierarchical
      Dirichlet processes. HDPs have some interesting applications in topic modeling and natural language
      processing, as well as time series modeling with hidden Markov model variants.
      If you like beta distributions, Dirichlet distributions, and Dirichlet processes,
      you'll love hierarchical Dirichlet processes, so get psyched!
    </p>
  </div>
    </div>
    </main>
    <footer>
      <div></div>
    </footer>
    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="js/dirichletprocesses.js"></script>
    <script src="js/blogpost.js"></script>
  </body>
</html>
